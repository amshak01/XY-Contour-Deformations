\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{biblatex}
\bibdata{writeup-blx,refs}
\citation{biblatex-control}
\abx@aux@refcontext{nty/global//global/global}
\citation{KT}
\abx@aux@cite{0}{KT}
\abx@aux@segm{0}{0}{KT}
\citation{Hasenbusch_2005}
\abx@aux@cite{0}{Hasenbusch_2005}
\abx@aux@segm{0}{0}{Hasenbusch_2005}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\citation{detmold2023signaltonoiseimprovementneuralnetwork}
\abx@aux@cite{0}{detmold2023signaltonoiseimprovementneuralnetwork}
\abx@aux@segm{0}{0}{detmold2023signaltonoiseimprovementneuralnetwork}
\citation{ronneberger2015unetconvolutionalnetworksbiomedical}
\abx@aux@cite{0}{ronneberger2015unetconvolutionalnetworksbiomedical}
\abx@aux@segm{0}{0}{ronneberger2015unetconvolutionalnetworksbiomedical}
\citation{perez2017filmvisualreasoninggeneral}
\abx@aux@cite{0}{perez2017filmvisualreasoninggeneral}
\abx@aux@segm{0}{0}{perez2017filmvisualreasoninggeneral}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}\protected@file@percent }
\citation{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\abx@aux@cite{0}{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\abx@aux@segm{0}{0}{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I}The XY-Model in 2D}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}The XY Hamiltonian}{6}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq:partition}{{2}{6}{The XY Hamiltonian}{equation.2.2}{}}
\citation{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\abx@aux@cite{0}{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\abx@aux@segm{0}{0}{drouintouchette2022kosterlitzthoulessphasetransitionintroduction}
\newlabel{eq:cont_ham}{{5}{7}{The XY Hamiltonian}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {b}Correlation Functions}{7}{subsubsection.2.1.2}\protected@file@percent }
\citation{Hasenbusch_2005}
\abx@aux@cite{0}{Hasenbusch_2005}
\abx@aux@segm{0}{0}{Hasenbusch_2005}
\citation{hanada2018markovchainmontecarlo}
\abx@aux@cite{0}{hanada2018markovchainmontecarlo}
\abx@aux@segm{0}{0}{hanada2018markovchainmontecarlo}
\@writefile{toc}{\contentsline {subsection}{\numberline {II}Lattice Field Theory}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Markov Chain Monte Carlo}{8}{subsubsection.2.2.1}\protected@file@percent }
\citation{hanada2018markovchainmontecarlo}
\abx@aux@cite{0}{hanada2018markovchainmontecarlo}
\abx@aux@segm{0}{0}{hanada2018markovchainmontecarlo}
\citation{PhysRevLett.62.361}
\abx@aux@cite{0}{PhysRevLett.62.361}
\abx@aux@segm{0}{0}{PhysRevLett.62.361}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {b}The Wolff Cluster Update}{9}{subsubsection.2.2.2}\protected@file@percent }
\citation{10.1007/978-3-030-43465-6_11}
\abx@aux@cite{0}{10.1007/978-3-030-43465-6_11}
\abx@aux@segm{0}{0}{10.1007/978-3-030-43465-6_11}
\citation{10.1007/978-3-030-43465-6_11}
\abx@aux@cite{0}{10.1007/978-3-030-43465-6_11}
\abx@aux@segm{0}{0}{10.1007/978-3-030-43465-6_11}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {c}Sign Problems}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration of a numerical sign problem (courtesy of \blx@tocontentsinit {0}\cite {10.1007/978-3-030-43465-6_11}) for the case of an observable which oscillates on the domain of integration. Small deviations in sampling may result in drastically different estimates of the observable mean.}}{10}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sign_problem}{{1}{10}{An illustration of a numerical sign problem (courtesy of \cite {10.1007/978-3-030-43465-6_11}) for the case of an observable which oscillates on the domain of integration. Small deviations in sampling may result in drastically different estimates of the observable mean}{figure.caption.6}{}}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Contour Deformations}{11}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}General Theory}{11}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{eq:undeformed}{{15}{11}{General Theory}{equation.3.15}{}}
\newlabel{eq:deformed}{{16}{11}{General Theory}{equation.3.16}{}}
\newlabel{eq:rewrite}{{17}{11}{General Theory}{equation.3.17}{}}
\newlabel{eq:variances}{{19}{11}{General Theory}{equation.3.19}{}}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\@input{contour1_cartoon.aux}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\citation{Detmold_2021}
\abx@aux@cite{0}{Detmold_2021}
\abx@aux@segm{0}{0}{Detmold_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Allowed and disallowed contours for a general variable (courtesy of \blx@tocontentsinit {0}\cite {Detmold_2021}) $\theta $ and for a periodic variable $\phi $. In the case of a periodic variable the slice along the imaginary axis at each end of the interval is identified, so vertical shift deformations are allowed.}}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:contours}{{2}{12}{Allowed and disallowed contours for a general variable (courtesy of \cite {Detmold_2021}) $\theta $ and for a periodic variable $\phi $. In the case of a periodic variable the slice along the imaginary axis at each end of the interval is identified, so vertical shift deformations are allowed}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {b}Vertical Shifts}{12}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{eq:Jac}{{20}{12}{Vertical Shifts}{equation.3.20}{}}
\newlabel{eq:deformed_ham}{{21}{13}{Vertical Shifts}{equation.3.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {c}Deforming the Correlator}{13}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{eq:ham_phase}{{24}{13}{Deforming the Correlator}{equation.3.24}{}}
\newlabel{eq:Poisson}{{27}{14}{Deforming the Correlator}{equation.3.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II}Machine Learning}{14}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Convolutional Networks}{14}{subsubsection.3.2.1}\protected@file@percent }
\@input{conv2d.aux}
\@input{transpose_conv2d.aux}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces How a 2x2 convolution kernel sweeps over a 3x3 input grid with periodic paddingto produce a new 3x3 grid. The first two iterations are shown. Notice the output grid is 3x3, since the kernel can be translated to the right (and down) 3 times before reaching the edge of the input. A stride of 2 for example would produce a 2x2 output}}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:conv2d}{{3}{15}{How a 2x2 convolution kernel sweeps over a 3x3 input grid with periodic paddingto produce a new 3x3 grid. The first two iterations are shown. Notice the output grid is 3x3, since the kernel can be translated to the right (and down) 3 times before reaching the edge of the input. A stride of 2 for example would produce a 2x2 output}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces How the transpose operation works by magnifying each site of the input image. Notice that the next iteration shown on the right will overlap the values 4 and 2 from the first iteration. After all iterations are performed, the resultant grids are summed together (with the blank spaces being zero)}}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:transconv2d}{{4}{16}{How the transpose operation works by magnifying each site of the input image. Notice that the next iteration shown on the right will overlap the values 4 and 2 from the first iteration. After all iterations are performed, the resultant grids are summed together (with the blank spaces being zero)}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {b}Multilayer Perceptrons}{16}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {c}The Input Mask}{17}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The input mask (left) with point sources at $(0,0)$ and $(16,16)$ on a $32\times 32$ lattice passes through a convolutional network. to produce a shift field which is "smeared out" across the entire lattice.}}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mask_to_shift}{{5}{17}{The input mask (left) with point sources at $(0,0)$ and $(16,16)$ on a $32\times 32$ lattice passes through a convolutional network. to produce a shift field which is "smeared out" across the entire lattice}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {d}1-Layer Network and Loss Function}{17}{subsubsection.3.2.4}\protected@file@percent }
\citation{ronneberger2015unetconvolutionalnetworksbiomedical}
\abx@aux@cite{0}{ronneberger2015unetconvolutionalnetworksbiomedical}
\abx@aux@segm{0}{0}{ronneberger2015unetconvolutionalnetworksbiomedical}
\newlabel{eq:loss}{{28}{18}{1-Layer Network and Loss Function}{equation.3.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {e}The U-Net, FiLM, and Generalized Loss}{18}{subsubsection.3.2.5}\protected@file@percent }
\citation{perez2017filmvisualreasoninggeneral}
\abx@aux@cite{0}{perez2017filmvisualreasoninggeneral}
\abx@aux@segm{0}{0}{perez2017filmvisualreasoninggeneral}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SiLU activation function}}{19}{figure.caption.11}\protected@file@percent }
\newlabel{fig:silu}{{6}{19}{SiLU activation function}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Top: The U-Net architecture we use in this project. Here we only depict the network with 3 levels to the "U", but it is easy to extend to an arbitrary number of levels, as long as the length and width of the layer remains integer, of course. At each level during the encoder step, a FiLM layer is applied to introduce a dependence on temperature. The final layer at each decoder step is saved and appended to the corresponding layer at each decoding step to preserve important long-range structure information. The 3rd dimension of each block in the diagram represents the number of channels, and is labeled in the first two levels. We double the number of channels at each encoder level, and halve them at each decoder level.   Bottom: The MLP implicit in each FiLM layer. The scalar input is the temperature and the output is two times the number of channels at each decoder level. Half of the outputs are used for scaling, the other half for bias.}}{20}{figure.caption.12}\protected@file@percent }
\newlabel{fig:unet}{{7}{20}{Top: The U-Net architecture we use in this project. Here we only depict the network with 3 levels to the "U", but it is easy to extend to an arbitrary number of levels, as long as the length and width of the layer remains integer, of course. At each level during the encoder step, a FiLM layer is applied to introduce a dependence on temperature. The final layer at each decoder step is saved and appended to the corresponding layer at each decoding step to preserve important long-range structure information. The 3rd dimension of each block in the diagram represents the number of channels, and is labeled in the first two levels. We double the number of channels at each encoder level, and halve them at each decoder level. \\ Bottom: The MLP implicit in each FiLM layer. The scalar input is the temperature and the output is two times the number of channels at each decoder level. Half of the outputs are used for scaling, the other half for bias}{figure.caption.12}{}}
\citation{albergo2025netsnonequilibriumtransportsampler}
\abx@aux@cite{0}{albergo2025netsnonequilibriumtransportsampler}
\abx@aux@segm{0}{0}{albergo2025netsnonequilibriumtransportsampler}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {f}Training Procedure}{21}{subsubsection.3.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{21}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Data Generation and Storage}{21}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {II}PyTorch}{21}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{21}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{21}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}KT-Theory}{21}{appendix.1.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}FCNs}{21}{appendix.1.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Statistics }{21}{appendix.1.C}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{17B313B315313BC17CA6822A0B25F92E}
\gdef \@abspage@last{22}
